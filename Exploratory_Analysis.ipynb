{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "INPUT_DIR = os.path.join(ROOT_DIR, 'considition-challenge')\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(INPUT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "if not os.path.exists(ROOT_DIR + '/Training_dataset'):\n",
    "    with zipfile.ZipFile(ROOT_DIR + '/dataset.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_data():\n",
    "    \n",
    "    import shutil\n",
    "    INPUT_DIR = os.path.join(ROOT_DIR, 'considition-challenge')\n",
    "    if os.path.exists(INPUT_DIR):\n",
    "        shutil.rmtree(INPUT_DIR)\n",
    "    \n",
    "\n",
    "    if not os.path.exists(INPUT_DIR):\n",
    "        #os.makedirs(INPUT_DIR)\n",
    "        try:\n",
    "            os.mkdir('../considition-challenge')\n",
    "            os.mkdir('../considition-challenge/train')\n",
    "            os.mkdir('../considition-challenge/validation')\n",
    "            os.mkdir('../considition-challenge/train/Images')\n",
    "            os.mkdir('../considition-challenge/train/Masks')\n",
    "            os.mkdir('../considition-challenge/train/Annotations')\n",
    "            os.mkdir('../considition-challenge/train/Percentages')\n",
    "            os.mkdir('../considition-challenge/validation/Images')\n",
    "            os.mkdir('../considition-challenge/validation/Masks')\n",
    "            os.mkdir('../considition-challenge/validation/Annotations')\n",
    "            os.mkdir('../considition-challenge/validation/Percentages')\n",
    "        except:\n",
    "            print(\"Error while creating directories\")\n",
    "\n",
    "recreate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.path.join(ROOT_DIR, 'Training_dataset/')\n",
    "destination_dir = os.path.join(ROOT_DIR, 'considition-challenge/')\n",
    "\n",
    "images_dir = os.path.join(source_dir, 'Images/')\n",
    "masks_dir = os.path.join(source_dir, 'Masks/all/')\n",
    "percentages_dir = os.path.join(source_dir, 'Percentages/')\n",
    "annotations_dir = os.path.join(source_dir, 'Annotations/')\n",
    "\n",
    "\n",
    "# Directory with training files\n",
    "train_images_dir = os.path.join(destination_dir, 'train/Images/')\n",
    "train_masks_dir = os.path.join(destination_dir, 'train/Masks/')\n",
    "train_percentages_dir = os.path.join(destination_dir, 'train/Percentages/')\n",
    "train_annotations_dir = os.path.join(destination_dir, 'train/Annotations/')\n",
    "\n",
    "\n",
    "# Directory with validation files\n",
    "validation_images_dir = os.path.join(destination_dir, 'validation/Images/')\n",
    "validation_masks_dir = os.path.join(destination_dir, 'validation/Masks/')\n",
    "validation_percentages_dir = os.path.join(destination_dir, 'validation/Percentages/')\n",
    "validation_annotations_dir = os.path.join(destination_dir, 'validation/Annotations/')\n",
    "\n",
    "\n",
    "print('There are {} images files' .format(len(os.listdir(images_dir))))\n",
    "print('There are {} masks files' .format(len(os.listdir(masks_dir))))\n",
    "print('There are {} percentages files' .format(len(os.listdir(percentages_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_percentages(percentages_dir, annotations_dir):\n",
    "    '''\n",
    "    \n",
    "    '''    \n",
    "    import pandas as pd\n",
    "    # percentages_dir = os.path.join(os.getcwd(),'Training_dataset/Percentages/')\n",
    "                               \n",
    "    # Obtain a list with the names of json files\n",
    "    json_names = []\n",
    "    for filename in os.listdir(percentages_dir):\n",
    "        json_names.append(filename)\n",
    "\n",
    "    percentages = []\n",
    "    results = {'imageName': '',\n",
    "             'buildingPercentage': '',\n",
    "             'roadPercentage':'',\n",
    "             'waterPercentage':''}\n",
    "\n",
    "    \n",
    "    for filename in json_names:\n",
    "        get_dir = os.path.join(percentages_dir, filename)\n",
    "        op = open(get_dir)\n",
    "        data = json.load(op)\n",
    "        op.close()\n",
    "    \n",
    "        file_name = os.path.splitext(filename)[0]+'.jpg'\n",
    "    \n",
    "        results = {}\n",
    "        results['imageName'] = file_name\n",
    "        results['buildingPercentage'] = data['building']\n",
    "        results['roadPercentage'] = data['road']\n",
    "        results['waterPercentage'] = data['water']\n",
    "        percentages.append(results)\n",
    "        \n",
    "    get_dir = os.path.join(annotations_dir, 'master_train.json')\n",
    "    op = open(get_dir)\n",
    "    data = json.load(op)\n",
    "    op.close()\n",
    "    \n",
    "    exc_img_size =[]\n",
    "    img_names = []\n",
    "    for item in data['images']:    \n",
    "        img_name = item['file_name']\n",
    "        wd = item['width']\n",
    "        hd = item['height']\n",
    "        if (wd!=hd) | (wd!=1024) | (hd!=1024):\n",
    "            exc_img_size.append(img_name)            \n",
    "\n",
    "    \n",
    "    df_results = pd.DataFrame.from_dict(percentages)\n",
    "    df_results['totalPercentage'] = df_results['buildingPercentage'] + df_results['roadPercentage'] + df_results['waterPercentage'] \n",
    "    list_exclude_files = list(df_results.loc[df_results['totalPercentage']==0,'imageName'])\n",
    "    list_exclude_sizes = list(exc_img_size)\n",
    "    list_exclude_files.extend(list_exclude_sizes)\n",
    "    \n",
    "    return percentages, df_results, list_exclude_files\n",
    "\n",
    "percentages, df_results, list_exclude_files = summarize_percentages(percentages_dir, annotations_dir)\n",
    "\n",
    "def fn_1(x):\n",
    "    if x in list_exclude_files:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df_results['x'] = df_results['imageName'].apply(lambda x: fn_1(x))\n",
    "df_results_clean = df_results.loc[df_results['x']==0,]\n",
    "del df_results_clean['x']\n",
    "\n",
    "df_x_ = df_results_clean.loc[:,['imageName']]\n",
    "df_y_ = df_results_clean.loc[:,['buildingPercentage','roadPercentage','waterPercentage']]\n",
    "\n",
    "df_y = df_y_.copy()\n",
    "df_x = df_x_.copy()\n",
    "\n",
    "df_y[\"buildingPercentage\"] = df_y['buildingPercentage'].transform(lambda x: 1 if x > 0 else 0)\n",
    "df_y[\"roadPercentage\"] = df_y['roadPercentage'].transform(lambda x: 1 if x > 0 else 0)\n",
    "df_y[\"waterPercentage\"] = df_y['waterPercentage'].transform(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "df_x['image'] = df_x['imageName'].transform(lambda x: x.split('.')[0])\n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df_x\n",
    "y = df_y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.01, random_state= 42, \n",
    "                                                    shuffle= True, stratify= y)\n",
    "train_list_jpg = X_train.imageName.values.tolist()\n",
    "val_list_jpg = X_test.imageName.values.tolist()\n",
    "\n",
    "train_list = X_train.image.values.tolist()\n",
    "val_list = X_test.image.values.tolist()\n",
    "\n",
    "print('There are {} training images' .format(len(train_list)))\n",
    "print('There are {} validation images' .format(len(val_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def copy_from_to_dir(source_dir, destination_dir, ext, filename):\n",
    "    this_file = source_dir + filename + ext\n",
    "    destination = destination_dir + filename + ext\n",
    "    copyfile(this_file, destination)\n",
    "\n",
    "def split_data(source_dir, train_list, val_list):       \n",
    "\n",
    "    training_set = train_list \n",
    "    validation_set = val_list\n",
    "\n",
    "    for filename in training_set:\n",
    "        copy_from_to_dir(images_dir, train_images_dir, '.jpg', filename)\n",
    "        copy_from_to_dir(masks_dir, train_masks_dir, '.png', filename)\n",
    "        copy_from_to_dir(percentages_dir, train_percentages_dir, '.json', filename)\n",
    "\n",
    "    for filename in validation_set:\n",
    "        copy_from_to_dir(images_dir, validation_images_dir, '.jpg', filename)\n",
    "        copy_from_to_dir(masks_dir, validation_masks_dir, '.png', filename)\n",
    "        copy_from_to_dir(percentages_dir, validation_percentages_dir, '.json', filename)\n",
    "        \n",
    "def split_json(image_names, annotations_dir, dest_annotations_dir, subset):\n",
    "    \n",
    "    # Load json from file\n",
    "    json_file = open(os.path.join(annotations_dir,'master_train.json'))\n",
    "    coco_json = json.load(json_file)\n",
    "    json_file.close()\n",
    "\n",
    "    # Get all images and add them to the dataset\n",
    "    images_id_list = []\n",
    "    seen_images = {}\n",
    "    for image in coco_json['images']:\n",
    "        image_id = image['id']\n",
    "        image_file_name = image['file_name']\n",
    "        if image_file_name in image_names:\n",
    "            if image_id not in seen_images:\n",
    "                seen_images[image_id] = image\n",
    "            images_id_list.append(image_id)\n",
    "    \n",
    "    new_img_json=[item for item in coco_json['images'] if item['id']  in images_id_list]\n",
    "    new_ann_json=[item for item in coco_json['annotations'] if item['image_id']  in images_id_list]\n",
    "    \n",
    "    new_json = {'images': '',\n",
    "                'annotations': '',\n",
    "                'info': '',\n",
    "                'categories': '',\n",
    "                'licenses': '' }\n",
    "    \n",
    "    new_json['info'] = coco_json['info']\n",
    "    new_json['categories'] = coco_json['categories']\n",
    "    new_json['licenses'] = coco_json['licenses']\n",
    "    new_json['images'] = new_img_json\n",
    "    new_json['annotations'] = new_ann_json                   \n",
    "                     \n",
    "    with open('{}.json' .format(subset), 'w') as outfile:\n",
    "        json.dump(new_json, outfile)\n",
    "    \n",
    "    if subset == 'train':\n",
    "        copy_from_to_dir(os.getcwd(), dest_annotations_dir, '.json', '/train')\n",
    "    if subset == 'validation':\n",
    "        copy_from_to_dir(os.getcwd(), dest_annotations_dir, '.json', '/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(images_dir, train_list, val_list)\n",
    "split_json(train_list_jpg, annotations_dir, train_annotations_dir, 'train')\n",
    "split_json(val_list_jpg, annotations_dir, validation_annotations_dir, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(ROOT_DIR)\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "from mrcnn.visualize import display_instances, display_images, apply_mask\n",
    "\n",
    "import skimage\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "\n",
    "class consid_dataset(utils.Dataset):\n",
    "    def load_data(self, dataset_dir, subset, class_ids=None,\n",
    "                  class_map=None, return_coco=False):\n",
    "        \"\"\"Load a subset of the COCO dataset.\n",
    "        dataset_dir: The root directory of the COCO dataset.\n",
    "        subset: What to load (train, val, minival, valminusminival)\n",
    "        class_ids: If provided, only loads images that have the given classes.\n",
    "        return_coco: If True, returns the COCO object.\n",
    "        \"\"\"\n",
    "\n",
    "        coco = COCO(\"{}/{}/Annotations/{}.json\".format(dataset_dir, subset, subset))\n",
    "        image_dir = \"{}/{}/Images\".format(dataset_dir, subset)\n",
    "\n",
    "        # Load all classes or a subset?\n",
    "        if not class_ids:\n",
    "            # All classes\n",
    "            class_ids = sorted(coco.getCatIds())\n",
    "        \n",
    "        # All images or a subset?\n",
    "        if class_ids:\n",
    "            image_ids = []\n",
    "            for id in class_ids:\n",
    "                image_ids.extend(list(coco.getImgIds(catIds=[id])))\n",
    "            # Remove duplicates\n",
    "            image_ids = list(set(image_ids))\n",
    "        else:\n",
    "            # All images\n",
    "            image_ids = list(coco.imgs.keys())\n",
    "\n",
    "        \n",
    "        # Add classes\n",
    "        for i in class_ids:\n",
    "            self.add_class(\"coco\", i, coco.loadCats(i)[0][\"name\"])\n",
    "        \n",
    "\n",
    "        # Add images\n",
    "        for i in image_ids:\n",
    "            self.add_image(\n",
    "                \"coco\", image_id=i,\n",
    "                path=os.path.join(image_dir, coco.imgs[i]['file_name']),\n",
    "                width=coco.imgs[i][\"width\"],\n",
    "                height=coco.imgs[i][\"height\"],\n",
    "                annotations=coco.loadAnns(coco.getAnnIds(\n",
    "                    imgIds=[i], catIds=class_ids, iscrowd=None)))\n",
    "        if return_coco:\n",
    "            return coco\n",
    "\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"coco\":\n",
    "            return super(consid_dataset2, self).load_mask(image_id)\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"coco.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation, image_info[\"height\"],\n",
    "                                   image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                # Is it a crowd? If so, use a negative class ID.\n",
    "                if annotation['iscrowd']:\n",
    "                    # Use negative class ID for crowds\n",
    "                    class_id *= -1\n",
    "                    # For crowd masks, annToMask() sometimes returns a mask\n",
    "                    # smaller than the given dimensions. If so, resize it.\n",
    "                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\n",
    "                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2).astype(np.bool)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            return mask, class_ids\n",
    "        else:\n",
    "            # Call super class to return an empty mask\n",
    "            return super(consid_dataset2, self).load_mask(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        '''\n",
    "        Return the path to the image.\n",
    "        '''\n",
    "        info = self.image_info[image_id]\n",
    "        return info['path']\n",
    "            \n",
    "    ######------\n",
    "    def display_image(self, image_id): #theres a display_images at visuzalise\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        # Load image\n",
    "        img = skimage.io.imread(self.image_info[image_id]['path'])\n",
    "        skimage.io.imshow(img)\n",
    "        skimage.io.show()\n",
    "    \n",
    "    \n",
    "    def image_data(self, image_id):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        info = self.image_info[image_id]\n",
    "        return info\n",
    "    \n",
    "    def conv_img_to_int_ID(self, num, subset):\n",
    "        \n",
    "        path_dir = os.path.abspath(\"../\")\n",
    "        if subset=='train':\n",
    "            REF = os.path.join(path_dir, 'considition-challenge/train/Annotations/train.json')\n",
    "        elif subset == 'validation':\n",
    "            REF = os.path.join(path_dir, 'considition-challenge/validation/Annotations/validation.json')\n",
    "        \n",
    "        # Load json from file\n",
    "        flnm = open(REF)\n",
    "        cc = json.load(flnm)\n",
    "        flnm.close()\n",
    "        \n",
    "        \n",
    "        images_id_list = []\n",
    "        seen_images = {}\n",
    "        for image in cc['images']:\n",
    "            image_id = image['id']\n",
    "            image_file_name = image['file_name']\n",
    "            if image_id not in seen_images:\n",
    "                seen_images[image_id] = image\n",
    "            images_id_list.append(image_id)        \n",
    "    \n",
    "        dictOfWords = { images_id_list[i]:i for i in range(0, len(images_id_list) ) }\n",
    "        #print(dictOfWords[num])\n",
    "        return dictOfWords[num]\n",
    "    ######------\n",
    "\n",
    "    # The following two functions are from pycocotools with a few changes.\n",
    "\n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# train set\n",
    "train_set = consid_dataset()\n",
    "train_set.load_data(INPUT_DIR, 'train')\n",
    "train_set.prepare()\n",
    "print('Train dataset: {}' .format(len(train_set.image_ids)))\n",
    "\n",
    "# test/val set\n",
    "val_set = consid_dataset()\n",
    "val_set.load_data(INPUT_DIR, 'validation')\n",
    "val_set.prepare()\n",
    "print('Test dataset: {}' .format(len(val_set.image_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "dataset = train_set\n",
    "image_ids = np.random.choice(dataset.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = val_set\n",
    "image_ids = np.random.choice(dataset.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "ia.seed(4)\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = val_set\n",
    "image_ids #= np.random.choice(dataset.image_ids, 4)\n",
    "print(image_ids)\n",
    "ids = []\n",
    "ids_path = []\n",
    "for image_id in image_ids:\n",
    "    image = dataset.load_image(image_id)\n",
    "    path = imageio.imread(dataset.image_reference(image_id))\n",
    "    ids.append(image)\n",
    "    ids_path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "images = ids\n",
    "rotate = iaa.Affine(rotate=(-25, 25))\n",
    "images_aug = rotate.augment_images(images)\n",
    "\n",
    "print(\"Augmented batch:\")\n",
    "ia.imshow(np.hstack(images_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    #iaa.Affine(rotate=(-25, 25)),\n",
    "    iaa.AdditiveGaussianNoise(scale=(30, 90)), #dont see much difference\n",
    "    #iaa.Crop(percent=(0, 0.2), keep_size=True) #useful\n",
    "])\n",
    "images_aug = seq.augment_images(images)\n",
    "\n",
    "print(\"Augmented batch:\")\n",
    "ia.imshow(np.hstack(images_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "            rotate=(-45, 45),\n",
    "            shear=(-30, 30)\n",
    "    )])\n",
    "\n",
    "images_aug = [seq.augment_image(image) for image in ids]\n",
    "\n",
    "print(\"Augmented:\")\n",
    "ia.imshow(ia.draw_grid(images_aug, cols=4, rows=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.AdditiveGaussianNoise(scale=(20, 30)),\n",
    "    iaa.Crop(percent=(0, 0.4)),\n",
    "    iaa.AddToHueAndSaturation((-60, 60)),  # change their color #useful too?\n",
    "    iaa.ElasticTransformation(alpha=90, sigma=9),  # water-like effect #useful test more\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images, #useful\n",
    "    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "    ])\n",
    "\n",
    "images_aug = [seq.augment_image(image) for image in ids]\n",
    "\n",
    "print(\"Augmented:\")\n",
    "ia.imshow(ia.draw_grid(images_aug, cols=4, rows=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "    # But we only blur about 50% of all images.\n",
    "    iaa.Sometimes(0.5,\n",
    "        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "    ),\n",
    "    # Strengthen or weaken the contrast in each image.\n",
    "    iaa.ContrastNormalization((0.75, 2)),\n",
    "    # Add gaussian noise.\n",
    "    # For 50% of all images, we sample the noise once per pixel.\n",
    "    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "    # channel. This can change the color (not only brightness) of the\n",
    "    # pixels.\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "    # Make some images brighter and some darker.\n",
    "    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "    # which can end up changing the color of the images.\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2)\n",
    "    ])\n",
    "\n",
    "images_aug = [seq.augment_image(image) for image in ids]\n",
    "\n",
    "print(\"Augmented:\")\n",
    "ia.imshow(ia.draw_grid(images_aug, cols=4, rows=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "\n",
    "    )])\n",
    "    \n",
    "\n",
    "images_aug = [seq.augment_image(image) for image in ids]\n",
    "\n",
    "print(\"Augmented:\")\n",
    "ia.imshow(ia.draw_grid(images_aug, cols=4, rows=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "\n",
    "                # Add gaussian noise to some images.\n",
    "                # In 50% of these cases, the noise is randomly sampled per\n",
    "                # channel and pixel.\n",
    "                # In the other 50% of all cases it is sampled once per\n",
    "                # pixel (i.e. brightness change).\n",
    "    #iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "\n",
    "\n",
    "                # Invert each image's channel with 5% probability.\n",
    "                # This sets each pixel value v to 255-v.\n",
    "    #iaa.Invert(0.05, per_channel=True), # invert color channels\n",
    "\n",
    "                # Change brightness of images (50-150% of original value).\n",
    "    #iaa.Multiply((0.5, 1.5), per_channel=0.5),\n",
    "\n",
    "                # Improve or worsen the contrast of images.\n",
    "    #iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n",
    "\n",
    "                # Convert each image to grayscale and then overlay the\n",
    "                # result with the original with random alpha. I.e. remove\n",
    "                # colors with varying strengths.\n",
    "    #iaa.Grayscale(alpha=(0.0, 1.0))\n",
    "])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "images_aug = [seq.augment_image(image) for image in ids]\n",
    "\n",
    "print(\"Augmented:\")\n",
    "ia.imshow(ia.draw_grid(images_aug, cols=4, rows=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "\n",
    "\n",
    "seq = iaa.Sequential([\n",
    "    \n",
    "    iaa.Sometimes(0.4,(\n",
    "        iaa.Affine(\n",
    "            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "            rotate=(-45, 45),\n",
    "            shear=(-16, 16)\n",
    "        ))),\n",
    "    iaa.ContrastNormalization((0.75, 2)),\n",
    "    iaa.Sometimes(0.6,(\n",
    "        #iaa.ContrastNormalization((0.5, 1.5)),\n",
    "        iaa.Crop(percent=(0, 0.4)),\n",
    "        iaa.AddToHueAndSaturation((-60, 60)),  # change their color\n",
    "        #iaa.ElasticTransformation(alpha=90, sigma=9),  # water-like effect\n",
    "        iaa.Fliplr(0.5), # horizontally flip 50% of the images,\n",
    "            \n",
    "        iaa.Sometimes(0.5,([\n",
    "            #iaa.GaussianBlur(sigma=(0, 0.5)),\n",
    "            iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "            #iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "            iaa.Multiply((0.8, 1.5), per_channel=0.4)\n",
    "        ])),\n",
    "        \n",
    "        iaa.Invert(0.05, per_channel=True), # invert color channels\n",
    "        iaa.Grayscale(alpha=(0.0, 1.0))\n",
    "        ))\n",
    "    ], random_order=True)\n",
    "\n",
    "# load images with different sizes\n",
    "images_different_sizes = ids\n",
    "\n",
    "# augment them as one batch\n",
    "images_aug = seq.augment_images(ids_path)\n",
    "\n",
    "# visualize the results\n",
    "print(\"Image 0 (input shape: %s, output shape: %s)\" % (images_different_sizes[0].shape, images_aug[0].shape))\n",
    "ia.imshow(np.hstack([images_different_sizes[0], images_aug[0]]))\n",
    "\n",
    "print(\"Image 1 (input shape: %s, output shape: %s)\" % (images_different_sizes[1].shape, images_aug[1].shape))\n",
    "ia.imshow(np.hstack([images_different_sizes[1], images_aug[1]]))\n",
    "\n",
    "print(\"Image 2 (input shape: %s, output shape: %s)\" % (images_different_sizes[2].shape, images_aug[2].shape))\n",
    "ia.imshow(np.hstack([images_different_sizes[2], images_aug[2]]))\n",
    "\n",
    "print(\"Image 3 (input shape: %s, output shape: %s)\" % (images_different_sizes[3].shape, images_aug[3].shape))\n",
    "ia.imshow(np.hstack([images_different_sizes[3], images_aug[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
